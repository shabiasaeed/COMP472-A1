{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a35eeb9f-df70-4ab1-a243-2d2025888eb0",
      "metadata": {},
      "source": [
        "# üêöüêß Penguin and Abalone Classifier üêßüêö\n",
        "Using different machine learning algorithms and different data sets, this notebook attempts to predict the species of a penguin given its features and the sex of an abalone given its features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41e8d4a-3314-4d22-b884-b2cb1f8a8e8c",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Before running this notebook, make sure to run the following command to install all of the required libraries needed to execute the code properly.\n",
        "\n",
        "`pip install -r requirements.txt --user`\n",
        "\n",
        "**NOTE:** However, if you installed your Python installation globally, e.g. inside C:\\Program Files\\Python311 (and not in C:\\User\\%Appdata\\Python311), remove the ***--user*** argument."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41e8d4a-3314-4d22-b884-b2cb1f8a8e8c",
      "metadata": {},
      "source": [
        "## Imports and Declarations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af459636",
      "metadata": {},
      "outputs": [],
      "source": [
        "# data analysis and manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# model\n",
        "from sklearn.tree import DecisionTreeClassifier as DTree\n",
        "from sklearn.neural_network import MLPClassifier as MLP\n",
        "\n",
        "# data postprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Tree visualization\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "\n",
        "# score tracking\n",
        "accuracy_scores = []\n",
        "macro_f1_scores = []\n",
        "weighted_f1_scores = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af459636",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3d6b3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read data from file\n",
        "penguin = pd.read_csv('penguins.csv')\n",
        "abalone = pd.read_csv('abalone.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3d6b3e",
      "metadata": {},
      "source": [
        "## Preprocessing - Penguin and Abalone Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a92a1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "penguin_encoded = penguin.apply(label_encoder.fit_transform)\n",
        "print (\"Raw Penguin Data\")\n",
        "print (penguin_encoded.head())\n",
        "print ()\n",
        "label_encoder = LabelEncoder()\n",
        "abalone_encoded = abalone.apply(label_encoder.fit_transform)\n",
        "print (\"Raw Abalone Data\")\n",
        "print (abalone_encoded.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a92a1f",
      "metadata": {},
      "source": [
        "## Preprocessing - One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1899c5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguin_hot_encoder = OneHotEncoder()\n",
        "penguin_hot_encoder.fit(penguin_encoded)\n",
        "penguin_hot_labels = penguin_hot_encoder.transform(penguin_encoded).toarray()\n",
        "print (\"Penguin data set shape: \", penguin_encoded.shape)\n",
        "print (\"Penguin hot labels shape: \", penguin_hot_labels.shape)\n",
        "print ()\n",
        "\n",
        "abalone_hot_encoder = OneHotEncoder()\n",
        "abalone_hot_encoder.fit(abalone_encoded)\n",
        "abalone_hot_labels = abalone_hot_encoder.transform(abalone_encoded).toarray()\n",
        "print (\"Abalone data set shape: \", abalone_encoded.shape)\n",
        "print (\"Abalone hot labels shape: \", abalone_hot_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1899c5b",
      "metadata": {},
      "source": [
        "## Preprocessing - Double Check for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee8f710",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Penguin data set - column with missing value:')\n",
        "print(penguin.isna().sum())\n",
        "print()\n",
        "print('Abalone data set - column with missing value:')\n",
        "print(abalone.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee8f710",
      "metadata": {},
      "source": [
        "## Charting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f39803",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploratopry Data Analysis\n",
        "sns.pairplot(penguin, corner=True)\n",
        "plt.savefig('penguin_pairplot.png')\n",
        "\n",
        "sns.pairplot(abalone, corner=True)\n",
        "plt.savefig('abalone_pairplot.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f39803",
      "metadata": {},
      "source": [
        "## Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "400cb0dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# split data into training (80%) and testing (20%) sets with random state 0\n",
        "penguinXtrain, penguinXtest, penguinYtrain, penguinYtest = train_test_split(penguin_encoded, penguin_hot_labels, test_size=0.2, random_state=0)\n",
        "print (\"Penguin total X train: \", len(penguinXtrain))\n",
        "print (\"Penguin total X test: \", len(penguinXtest))\n",
        "print (\"Penguin total Y train: \", len(penguinYtrain))\n",
        "print (\"Penguin total Y test: \", len(penguinYtest))\n",
        "print()\n",
        "abaloneXtrain, abaloneXtest, abaloneYtrain, abaloneYtest = train_test_split(abalone_encoded, abalone_hot_labels, test_size=0.2, random_state=0)\n",
        "print (\"Abalone total X train: \", len(abaloneXtrain))\n",
        "print (\"Abalone total X test: \", len(abaloneXtest))\n",
        "print (\"Abalone total Y train: \", len(abaloneYtrain))\n",
        "print (\"Abalone total Y test: \", len(abaloneYtest))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533d7c3c",
      "metadata": {},
      "source": [
        "## Base Decision Tree (Penguin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284a475e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Penguin\n",
        "BasePenguinDtree =  DecisionTreeClassifier() # Create a decision tree classifier with default values\n",
        "BasePenguinDtree.fit(penguinXtrain, penguinYtrain) # Fit the model with training data\n",
        "\n",
        "# To check for accuracy, use the score function.\n",
        "accuracy = BasePenguinDtree.score(penguinXtest, penguinYtest)\n",
        "print(\"Base Penguin Decision Tree Accuracy: \", accuracy)\n",
        "\n",
        "plt.figure(figsize=(20,10))  # Adjust the figure size as needed\n",
        "plot_tree(BasePenguinDtree, filled=True, rounded=True, class_names=['Adelie', 'Chinstrap', 'Gentoo'], feature_names=penguin.columns,fontsize=10) # Plot the tree\n",
        "plt.show() # Show the plot\n",
        "plt.savefig('Penguin_BDT.png') # Save the plot to a file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Decision Tree (Abalone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Abalone\n",
        "BaseAbaloneDtree =  DecisionTreeClassifier() # Create a decision tree classifier with default values\n",
        "BaseAbaloneDtree.fit(abaloneXtrain, abaloneYtrain) # Fit the model with training data\n",
        "\n",
        "# To check for accuracy, use the score function.\n",
        "accuracy_abalone = BaseAbaloneDtree.score(abaloneXtest, abaloneYtest)\n",
        "print(\"Base Abalone Decision Tree Accuracy: \", accuracy_abalone)\n",
        "\n",
        "plt.figure(figsize=(20,10),dpi=300)  # Adjust the figure size as needed\n",
        "plot_tree(BaseAbaloneDtree, filled=True, rounded=True, class_names=['Adelie', 'Chinstrap', 'Gentoo'], feature_names=abalone.columns,fontsize=10,max_depth=3) # Plot the tree\n",
        "plt.show() # Show the plot\n",
        "plt.savefig('Abalone_BDT.png') # Save the plot to a file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a41c0d",
      "metadata": {},
      "source": [
        "## Top Decision Tree (Penguin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20bc949",
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE HERE\n",
        "# Decision Tree Classifier\n",
        "# For Penguin\n",
        "grid_params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [3, 5,None],   # Define the different values of max depth to try\n",
        "    'min_samples_split': [2, 5, 10], # Define the different values of min samples split to try\n",
        "}\n",
        "\n",
        "TopPenguinDtree = DecisionTreeClassifier() # Create a decision tree classifier with default values\n",
        "grid_search = GridSearchCV(TopPenguinDtree, grid_params, cv=5,scoring='accuracy') # Create a grid search object 5-fold cross validation and accuracy as the scoring method\n",
        "grid_search.fit(penguinXtrain, penguinYtrain) # Fit the grid search with training data\n",
        "\n",
        "TopPenguinDtree_best_params = grid_search.best_params_ # Get the best parameters\n",
        "TopPenguinDtree_best_dtree = grid_search.best_estimator_ # Get the best estimator\n",
        "\n",
        "print(\"Best parameters for Penguin Decision Tree: \", TopPenguinDtree_best_params) # Print the best parameters\n",
        "\n",
        "# visualize the best decision tree\n",
        "plt.figure(figsize=(20,10),dpi=300)  # Adjust the figure size as needed\n",
        "plot_tree(TopPenguinDtree_best_dtree, filled=True, rounded=True, class_names=['Adelie', 'Chinstrap', 'Gentoo'], feature_names=penguin.columns,fontsize=10,max_depth=3) # Plot the tree\n",
        "plt.show() # Show the plot\n",
        "plt.savefig('Penguin_BDT.png') # Save the plot to a file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f91ccd1",
      "metadata": {},
      "source": [
        "## Top Decision Tree (Abalone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960150c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE HERE\n",
        "# For Abalone\n",
        "grid_params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [3, 5,7],   # Define the different values of max depth to try\n",
        "    'min_samples_split': [2, 5] # Define the different values of min samples split to try\n",
        "}   \n",
        "TopAbaloneDtree = DecisionTreeClassifier() # Create a decision tree classifier with default values\n",
        "grid_search_abalone = GridSearchCV(TopAbaloneDtree, grid_params, cv=5,scoring='accuracy') # Create a grid search object 5-fold cross validation and accuracy as the scoring method\n",
        "grid_search_abalone.fit(abaloneXtrain, abaloneYtrain) # Fit the grid search with training data\n",
        "\n",
        "TopAbaloneDtree_best_params = grid_search_abalone.best_params_ # Get the best parameters\n",
        "TopAbaloneDtree_best_dtree = grid_search_abalone.best_estimator_ # Get the best estimator\n",
        "\n",
        "print(\"Best parameters for Abalone Decision Tree: \", TopAbaloneDtree_best_params) # Print the best parameters\n",
        "\n",
        "# visualize the best decision tree\n",
        "plt.figure(figsize=(20,10),dpi=300)  # Adjust the figure size as needed\n",
        "plot_tree(TopAbaloneDtree_best_dtree, filled=True, rounded=True, class_names=['Adelie', 'Chinstrap', 'Gentoo'], feature_names=abalone.columns,fontsize=10,max_depth=3) # Plot the tree\n",
        "plt.show() # Show the plot\n",
        "plt.savefig('Abalone_BDT.png') # Save the plot to a file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "724f0b68",
      "metadata": {},
      "source": [
        "## Base MLP (Penguin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc42be18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base MLP For Penguin\n",
        "\n",
        "# change string value to numeric\n",
        "penguin['species'] = pd.Categorical(penguin['species']).codes\n",
        "penguin['island'] = pd.Categorical(penguin['island']).codes\n",
        "penguin['sex'] = pd.Categorical(penguin['sex']).codes\n",
        "\n",
        "#change dataframe to array\n",
        "penguin_array = penguin.values\n",
        "\n",
        "#split x and y (feature and target)\n",
        "penguinXtrain = penguin_array[:,1:]\n",
        "penguinYtrain = penguin_array[:,0]\n",
        "\n",
        "#standardize\n",
        "#palmer-penguin dataset has varying scales\n",
        "scaler = StandardScaler()\n",
        "penguinXtrain = scaler.fit_transform(penguinXtrain)\n",
        "\n",
        "base_mlp_penguin = MLP(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', random_state=0)\n",
        "\n",
        "# fit the model with training data\n",
        "base_mlp_penguin.fit(penguinXtrain, penguinYtrain)\n",
        "\n",
        "# predicting the species of penguins using the trained model\n",
        "base_mlp_penguin_predicted = base_mlp_penguin.predict(penguinXtest)\n",
        "\n",
        "# calculate scores\n",
        "accuracy = accuracy_score(penguinYtest, base_mlp_penguin_predicted)\n",
        "macro_f1 = f1_score(penguinYtest, base_mlp_penguin_predicted, average='macro')\n",
        "weighted_f1 = f1_score(penguinYtest, base_mlp_penguin_predicted, average='weighted')\n",
        "\n",
        "print(\"Base MLP Accuracy for Penguin: \", accuracy)\n",
        "print(\"Base MLP Macro-average F1 for Penguin: \", macro_f1)\n",
        "print(\"Base MLP Weighted-average F1 for Penguin: \", weighted_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bec91fe",
      "metadata": {},
      "source": [
        "## Top MLP (Penguin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3737ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top MLP For Penguin\n",
        "\n",
        "# change string value to numeric\n",
        "penguin['species'] = pd.Categorical(penguin['species']).codes\n",
        "penguin['island'] = pd.Categorical(penguin['island']).codes\n",
        "penguin['sex'] = pd.Categorical(penguin['sex']).codes\n",
        "\n",
        "#change dataframe to array\n",
        "penguin_array = penguin.values\n",
        "\n",
        "#split x and y (feature and target)\n",
        "penguinXtrain = penguin_array[:,1:]\n",
        "penguinYtrain = penguin_array[:,0]\n",
        "\n",
        "#standardize\n",
        "#palmer-penguin dataset has varying scales\n",
        "scaler = StandardScaler()\n",
        "penguinXtrain = scaler.fit_transform(penguinXtrain)\n",
        "\n",
        "# Define the different values of hyperparameters to try\n",
        "param_grid = {\n",
        "    'activation': ['sigmoid', 'tanh', 'relu'],\n",
        "    'hidden_layer_sizes': [(30, 50), (10, 10, 10), (100,)],\n",
        "    'solver': ['adam', 'sgd']\n",
        "}\n",
        "\n",
        "# Create a MLP classifier with default values\n",
        "mlp_penguin = MLP(random_state=0)\n",
        "\n",
        "# Create a grid search object\n",
        "grid_search = GridSearchCV(mlp_penguin, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to find the best parameters\n",
        "grid_search.fit(penguinXtrain, penguinYtrain)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_mlp_penguin = grid_search.best_params_\n",
        "best_mlp_penguin = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best parameters for Penguin MLP: \", best_params_mlp_penguin)\n",
        "\n",
        "# Predict the species of penguins using the trained model\n",
        "top_mlp_penguin_predicted = best_mlp_penguin.predict(penguinXtest)\n",
        "\n",
        "# Calculate scores\n",
        "accuracy = accuracy_score(penguinYtest, top_mlp_penguin_predicted)\n",
        "macro_f1 = f1_score(penguinYtest, top_mlp_penguin_predicted, average='macro')\n",
        "weighted_f1 = f1_score(penguinYtest, top_mlp_penguin_predicted, average='weighted')\n",
        "\n",
        "print(\"Top MLP Accuracy for Penguin: \", accuracy)\n",
        "print(\"Top MLP Macro-average F1 for Penguin: \", macro_f1)\n",
        "print(\"Top MLP Weighted-average F1 for Penguin: \", weighted_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base MLP (Abalone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base MLP For Abalone\n",
        "\n",
        "# change string value to numeric\n",
        "abalone['Type'] = pd.Categorical(abalone['Type']).codes\n",
        "\n",
        "# change dataframe to array\n",
        "abalone_array = abalone.values\n",
        "\n",
        "# split x and y (feature and target)\n",
        "abaloneXtrain = abalone_array[:,1:]\n",
        "abaloneYtrain = abalone_array[:,0]\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "abaloneXtrain = scaler.fit_transform(abaloneXtrain)\n",
        "\n",
        "base_mlp_abalone = MLP(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', random_state=0)\n",
        "\n",
        "# fit the model with training data\n",
        "base_mlp_abalone.fit(abaloneXtrain, abaloneYtrain)\n",
        "\n",
        "# predicting the species of abalone using the trained model\n",
        "base_mlp_abalone_predicted = base_mlp_abalone.predict(abaloneXtest)\n",
        "\n",
        "# calculate scores\n",
        "accuracy_abalone = accuracy_score(abaloneYtest, base_mlp_abalone_predicted)\n",
        "macro_f1_abalone = f1_score(abaloneYtest, base_mlp_abalone_predicted, average='macro')\n",
        "weighted_f1_abalone = f1_score(abaloneYtest, base_mlp_abalone_predicted, average='weighted')\n",
        "\n",
        "print(\"Base MLP Accuracy for Abalone: \", accuracy_abalone)\n",
        "print(\"Base MLP Macro-average F1 for Abalone: \", macro_f1_abalone)\n",
        "print(\"Base MLP Weighted-average F1 for Abalone: \", weighted_f1_abalone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top MLP (Abalone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top MLP For Abalone\n",
        "\n",
        "# change string value to numeric\n",
        "abalone['Type'] = pd.Categorical(abalone['Type']).codes\n",
        "\n",
        "# change dataframe to array\n",
        "abalone_array = abalone.values\n",
        "\n",
        "# split x and y (feature and target)\n",
        "abaloneXtrain = abalone_array[:,1:]\n",
        "abaloneYtrain = abalone_array[:,0]\n",
        "\n",
        "# Define the different values of hyperparameters to try\n",
        "param_grid = {\n",
        "    'activation': ['sigmoid', 'tanh', 'relu'],\n",
        "    'hidden_layer_sizes': [(30, 50), (10, 10, 10), (100,)],\n",
        "    'solver': ['adam', 'sgd']\n",
        "}\n",
        "\n",
        "# Create a MLP classifier with default values\n",
        "mlp_abalone = MLP(random_state=0)\n",
        "\n",
        "# Create a grid search object\n",
        "grid_search = GridSearchCV(mlp_abalone, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to find the best parameters\n",
        "grid_search.fit(abaloneXtrain, abaloneYtrain)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_mlp_abalone = grid_search.best_params_\n",
        "best_mlp_abalone = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best parameters for Abalone MLP: \", best_params_mlp_abalone)\n",
        "\n",
        "# Predict the species of abalone using the trained model\n",
        "top_mlp_abalone_predicted = best_mlp_abalone.predict(abaloneXtest)\n",
        "\n",
        "# Calculate scores\n",
        "accuracy = accuracy_score(abaloneYtest, top_mlp_abalone_predicted)\n",
        "macro_f1 = f1_score(abaloneYtest, top_mlp_abalone_predicted, average='macro')\n",
        "weighted_f1 = f1_score(abaloneYtest, top_mlp_abalone_predicted, average='weighted')\n",
        "\n",
        "print(\"Top MLP Accuracy for Abalone: \", accuracy)\n",
        "print(\"Top MLP Macro-average F1 for Abalone: \", macro_f1)\n",
        "print(\"Top MLP Weighted-average F1 for Abalone: \", weighted_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0701b103",
      "metadata": {},
      "source": [
        "## Run Model n Times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd97c50f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_multiple_times (num_runs, model, trueset, predictedset):\n",
        "  \"\"\"\n",
        "  Runs the model n times and returns the accuracy, macro-average F1, and weighted-average F1 scores.\n",
        "  \"\"\"\n",
        "  for i in range(num_runs):\n",
        "    match model:\n",
        "      case 'basemlp-abalone':\n",
        "        print (f\"Running Base-MLP Model {num_runs} times.\")\n",
        "        base_mlp_abalone.n_iter_ = 5\n",
        "        base_mlp_abalone.fit(abaloneXtrain, abaloneYtrain)\n",
        "        predictedset = base_mlp_abalone.predict(abaloneXtest)\n",
        "      case 'topmlp-abalone':\n",
        "        print (f\"Running Top-MLP Model {num_runs} times.\")\n",
        "        mlp_abalone.n_iter_ = 5\n",
        "        mlp_abalone.fit(abaloneXtrain, abaloneYtrain)\n",
        "        predictedset = mlp_abalone.predict(abaloneXtest)\n",
        "      case 'basemlp-penguin':\n",
        "        print (f\"Running Base-MLP Model {num_runs} times.\")\n",
        "        base_mlp_penguin.n_iter_ = 5\n",
        "        base_mlp_penguin.fit(penguinXtrain, penguinYtrain)\n",
        "        predictedset = base_mlp_penguin.predict(penguinXtest)\n",
        "      case 'topmlp-penguin':\n",
        "        print (f\"Running Top-MLP Model {num_runs} times.\")\n",
        "        mlp_penguin.n_iter_ = 5\n",
        "        mlp_penguin.fit(penguinXtrain, penguinYtrain)\n",
        "        predictedset = mlp_penguin.predict(penguinXtest)\n",
        "      case 'BasePenguinDtree':\n",
        "        print (f\"Running Base-Decision Tree Model {num_runs} times.\")\n",
        "        BasePenguinDtree.fit(penguinXtrain, penguinYtrain)\n",
        "        predictedset = BasePenguinDtree.predict(penguinXtest)\n",
        "      case 'TopPenguinDtree':\n",
        "        print (f\"Running Top-Decision Tree Model {num_runs} times.\")\n",
        "        TopPenguinDtree_best_dtree.fit(penguinXtrain, penguinYtrain)\n",
        "        predictedset = TopPenguinDtree_best_dtree.predict(penguinXtest)\n",
        "      case 'BaseAbaloneDtree':\n",
        "        print (f\"Running Base-Decision Tree Model {num_runs} times.\")\n",
        "        BaseAbaloneDtree.fit(abaloneXtrain, abaloneYtrain)\n",
        "        predictedset = BaseAbaloneDtree.predict(abaloneXtest)\n",
        "      case 'TopAbaloneDtree':\n",
        "        print (f\"Running Top-Decision Tree Model {num_runs} times.\")\n",
        "        TopAbaloneDtree_best_dtree.fit(abaloneXtrain, abaloneYtrain)\n",
        "        predictedset = TopAbaloneDtree_best_dtree.predict(abaloneXtest)\n",
        "\n",
        "    # calculate accuracy, macro-average F1, and weighted-average F1\n",
        "    accuracy = accuracy_score(trueset, predictedset)\n",
        "    macro_f1 = f1_score(trueset, predictedset, average='macro')\n",
        "    weighted_f1 = f1_score(trueset, predictedset, average='weighted')\n",
        "\n",
        "    # append the scores to the corresponding lists\n",
        "    accuracy_scores.append(accuracy)\n",
        "    macro_f1_scores.append(macro_f1)\n",
        "    weighted_f1_scores.append(weighted_f1)\n",
        "\n",
        "  return accuracy, macro_f1, weighted_f1, accuracy_scores, macro_f1_scores, weighted_f1_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89773d0",
      "metadata": {},
      "source": [
        "## Output Macro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cb510a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def output_macro (trueset, predictedset, model, dataset, hyperparameter, structure):\n",
        "  accuracy, macro_f1, weighted_f1, accuracy_scores, macro_f1_scores, weighted_f1_scores = run_multiple_times(5, model, trueset, predictedset)\n",
        "  with open(f'{dataset}-performance.txt', 'w') as f:\n",
        "    f.write(f'---==={structure} {hyperparameter}===---\\n')\n",
        "\n",
        "    confusion_matrix = np.array2string(confusion_matrix(trueset, predictedset))\n",
        "    f.write('---Section 4b---\\n')\n",
        "    f.write(f'{structure} Confusion Matrix:\\n')\n",
        "    f.write(confusion_matrix)\n",
        "    f.write('\\n')\n",
        "\n",
        "    # calculate precision, recall, and F1-measure for each class\n",
        "    classification_report = classification_report(trueset, predictedset)\n",
        "    f.write('---Section 4c---\\n')\n",
        "    f.write(f'{structure} Classification Report:\\n')\n",
        "    f.write(classification_report)\n",
        "    f.write('\\n')\n",
        "    \n",
        "    # calculate accuracy, macro-average F1, and weighted-average F1\n",
        "    accuracy = accuracy_score(trueset, predictedset)\n",
        "    macro_f1 = f1_score(trueset, predictedset, average='macro')\n",
        "    weighted_f1 = f1_score(trueset, predictedset, average='weighted')\n",
        "    f.write('---Section 4d---\\n')\n",
        "    print(f\"Accuracy: {accuracy}\\n\")\n",
        "    print(f\"Macro-average F1: {macro_f1}\\n\")\n",
        "    print(f\"Weighted-average F1: {weighted_f1}\\n\")\n",
        "\n",
        "    # calculate average accuracy and variance\n",
        "    accuracy_mean = np.mean(accuracy_scores)\n",
        "    accuracy_var = np.var(accuracy_scores)\n",
        "    f.write('---Section 5a---\\n')\n",
        "    f.write(f\"Average accuracy: {accuracy_mean}\\n\")\n",
        "    f.write(f\"Accuracy variance: {accuracy_var}\\n\")\n",
        "\n",
        "    # calculate average macro-average F1 and variance\n",
        "    macro_f1_mean = np.mean(macro_f1_scores)\n",
        "    macro_f1_var = np.var(macro_f1_scores)\n",
        "    f.write('---Section 5b---\\n')\n",
        "    f.write(f\"Average macro-average F1: {macro_f1_mean}\\n\")\n",
        "    f.write(f\"Macro-average F1 variance: {macro_f1_var}\\n\")\n",
        "\n",
        "    # calculate average weighted-average F1 and variance\n",
        "    weighted_f1_mean = np.mean(weighted_f1_scores)\n",
        "    weighted_f1_var = np.var(weighted_f1_scores)\n",
        "    f.write('---Section 5c---\\n')\n",
        "    f.write(f\"Average weighted-average F1: {weighted_f1_mean}\\n\")\n",
        "    f.write(f\"Weighted-average F1 variance: {weighted_f1_var}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1719cb37",
      "metadata": {},
      "source": [
        "## Output Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43747abf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output Base DT Statistics (Penguin)\n",
        "output_macro(penguinYtest, BasePenguinDtree.predict(penguinXtest), 'BasePenguinDtree', 'penguin', 'default', 'Base-DT Penguin')\n",
        "\n",
        "# Output Top DT Statistics (Penguin)\n",
        "output_macro(penguinYtest, TopPenguinDtree.predict(penguinXtest), 'TopPenguinDtree', 'penguin', TopPenguinDtree_best_params, 'Top-DT Penguin')\n",
        "\n",
        "# Output Base DT Statistics (Abalone)\n",
        "output_macro(abaloneYtest, BaseAbaloneDtree.predict(abaloneXtest), 'BaseAbaloneDtree', 'abalone', 'default', 'Base-DT Abalone')\n",
        "\n",
        "# Output Top DT Statistics (Abalone)\n",
        "output_macro(abaloneYtest, TopAbaloneDtree.predict(abaloneXtest), 'TopAbaloneDtree', 'abalone', TopAbaloneDtree_best_params, 'Top-DT Abalone')\n",
        "\n",
        "# Output Base MLP Statistics (Penguin)\n",
        "output_macro(penguinYtest, base_mlp_penguin_predicted, 'base_mlp_penguin', 'penguin', 'default, logistic, sgd', 'Base-MLP Penguin')\n",
        "\n",
        "# Output Top MLP Statistics (Penguin)\n",
        "output_macro(penguinYtest, top_mlp_penguin_predicted, 'mlp_penguin', 'penguin', best_params_mlp_penguin, 'Top-MLP Penguin')\n",
        "\n",
        "# Output Base MLP Statistics (Abalone)\n",
        "output_macro(abaloneYtest, base_mlp_abalone_predicted, 'base_mlp_abalone', 'abalone', 'default, logistic, sgd', 'Base-MLP Abalone')\n",
        "\n",
        "# Output Top MLP Statistics (Abalone)\n",
        "output_macro(abaloneYtest, top_mlp_abalone_predicted, 'mlp_abalone', 'abalone', best_params_mlp_abalone, 'Top-MLP Abalone')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
